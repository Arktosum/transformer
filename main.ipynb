{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"train.parquet\")  # If using local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "english_sentences = []\n",
    "deutsch_sentences = []\n",
    "for i in range(len(df))[:100]:\n",
    "    english = df.iloc[i].iloc[0]['en']\n",
    "    deutsch = df.iloc[i].iloc[0]['de']\n",
    "    english_sentences.append(english)\n",
    "    deutsch_sentences.append(deutsch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BPETokenizer(corpus_sentences,vocab_size=30000):\n",
    "    # Initialize a BPE tokenizer\n",
    "    tokenizer = Tokenizer(BPE())\n",
    "\n",
    "    # Set up a trainer with desired vocabulary size\n",
    "    trainer = BpeTrainer(vocab_size=vocab_size, min_frequency=2, special_tokens=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "\n",
    "    # Define a pre-tokenizer to split input text into words\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    # Tokenizer expects an iterator of strings\n",
    "    tokenizer.train_from_iterator(corpus_sentences, trainer=trainer)\n",
    "    tokenizer.enable_padding(length=GLOBALS['CONTEXT-SIZE'], pad_id=tokenizer.token_to_id(\"<pad>\"), pad_token=\"<pad>\")\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "GLOBALS = {\n",
    "    \"INPUT-VOCABULARY-SIZE\" : 30_000, # number of accepted distinct INPUT tokens.\n",
    "    \"OUTPUT-VOCABULARY-SIZE\" : 30_000, # number of accepted distinct OUTPUTtokens.\n",
    "    'INPUT-EMBEDDING-DIMENSION' : 4096 ,# dimension of each embedding vector for a token,\n",
    "    \"CONTEXT-SIZE\" : 512, # Fixed length of an input sequence\n",
    "}\n",
    "english_encoder = BPETokenizer(english_sentences,GLOBALS[\"INPUT-VOCABULARY-SIZE\"])\n",
    "deutsch_encoder = BPETokenizer(deutsch_sentences,GLOBALS[\"OUTPUT-VOCABULARY-SIZE\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class InputEmbeddingLayer(nn.Module):\n",
    "    def __init__(self,vocabulary_size,embedding_vector_dimension):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The input to the module is a list of indices, and the output is the corresponding word embeddings.\n",
    "        # num_embeddings (int) – size of the dictionary of embeddings\n",
    "        # embedding_dim (int) – the size of each embedding vector\n",
    "        self.embedding = nn.Embedding(vocabulary_size,embedding_vector_dimension)\n",
    "\n",
    "\n",
    "    def forward(self,tokens):\n",
    "        '''\n",
    "        Output shape : (len(tokens),embedding_dim)\n",
    "        '''\n",
    "        return self.embedding(tokens)\n",
    "    \n",
    "\n",
    "class PositionalEncodingLayer(nn.Module):\n",
    "    def __init__(self,sequence_length,embedding_vector_dimension):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoding = torch.zeros(sequence_length, embedding_vector_dimension)\n",
    "    \n",
    "        # Create a position tensor\n",
    "        position = torch.arange(0, sequence_length, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Compute the div_term\n",
    "        div_term = torch.exp(torch.arange(0, embedding_vector_dimension, 2, dtype=torch.float) *\n",
    "                            -(math.log(10000.0) / embedding_vector_dimension))\n",
    "        \n",
    "        # Apply the sinusoidal functions\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "    def forward(self,embeddings):\n",
    "        # Shape : sequence , N_embed_dim\n",
    "        return embeddings + self.encoding\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0745,  0.0561,  1.1936,  ..., -0.0871, -0.8049, -1.3031],\n",
      "        [ 0.2833, -0.7635,  3.2787,  ...,  1.4501,  1.4383, -0.7224],\n",
      "        [ 0.6348, -0.2284,  1.2468,  ...,  0.0390,  0.3357,  1.4654],\n",
      "        ...,\n",
      "        [-0.8920,  1.7560, -2.1955,  ...,  0.5288,  0.2063,  0.7464],\n",
      "        [-0.0806,  1.2451, -2.3415,  ...,  0.5288,  0.2064,  0.7464],\n",
      "        [-0.0721,  0.2863, -1.6291,  ...,  0.5288,  0.2065,  0.7464]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "input_embedding_layer = InputEmbeddingLayer(GLOBALS['INPUT-VOCABULARY-SIZE'],GLOBALS['INPUT-EMBEDDING-DIMENSION'])\n",
    "\n",
    "positional_encoding_layer = PositionalEncodingLayer(GLOBALS['CONTEXT-SIZE'],GLOBALS['INPUT-EMBEDDING-DIMENSION'])\n",
    "\n",
    "for english_sentence,deutsch_sentence in zip(english_sentences,deutsch_sentences):\n",
    "    \n",
    "    english_encoding = english_encoder.encode(english_sentence).ids\n",
    "    deutsch_encoding = deutsch_encoder.encode(deutsch_sentence).ids\n",
    "    \n",
    "    input_embeddings = input_embedding_layer.forward(torch.LongTensor(english_encoding))\n",
    "    \n",
    "    input_embeddings = positional_encoding_layer.forward(input_embeddings)\n",
    "    print(input_embeddings)\n",
    "    # print(\"English: \",english_encoding)\n",
    "    # print(\"Deutsch: \",deutsch_encoding)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
