{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"./test-00000-of-00001.parquet\")  # If using local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "english_sentences = []\n",
    "deutsch_sentences = []\n",
    "for i in range(len(df))[:100]:\n",
    "    english = df.iloc[i].iloc[0]['en']\n",
    "    deutsch = df.iloc[i].iloc[0]['de']\n",
    "    english_sentences.append(english)\n",
    "    deutsch_sentences.append(deutsch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BPETokenizer(corpus_sentences,vocab_size=30000):\n",
    "    # Initialize a BPE tokenizer\n",
    "    tokenizer = Tokenizer(BPE())\n",
    "\n",
    "    # Set up a trainer with desired vocabulary size\n",
    "    trainer = BpeTrainer(vocab_size=vocab_size, min_frequency=2, special_tokens=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "\n",
    "    # Define a pre-tokenizer to split input text into words\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    # Tokenizer expects an iterator of strings\n",
    "    tokenizer.train_from_iterator(corpus_sentences, trainer=trainer)\n",
    "    tokenizer.enable_padding(length=GLOBALS['CONTEXT-SIZE'], pad_id=tokenizer.token_to_id(\"<pad>\"), pad_token=\"<pad>\")\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "GLOBALS = {\n",
    "    \"INPUT-VOCABULARY-SIZE\" : 30_000, # number of accepted distinct INPUT tokens.\n",
    "    \"OUTPUT-VOCABULARY-SIZE\" : 30_000, # number of accepted distinct OUTPUTtokens.\n",
    "    'INPUT-EMBEDDING-DIMENSION' : 4096 ,# dimension of each embedding vector for a token,\n",
    "    \"CONTEXT-SIZE\" : 512, # Fixed length of an input sequence\n",
    "    \"ATTENTION-HEAD-COUNT\" : 4, # Number of attention heads to use. should properly divide input-embedding-dimension\n",
    "    \"FFN-HIDDEN-DIMENSION\" : 4096 * 2, # Dimension of hidden layer in FFN ,Usually higher than Embedding Dimension.\n",
    "    \"ENCODER-BLOCK-COUNT\" : 4, # Number of blocks of stacked encoders\n",
    "}\n",
    "english_encoder = BPETokenizer(english_sentences,GLOBALS[\"INPUT-VOCABULARY-SIZE\"])\n",
    "deutsch_encoder = BPETokenizer(deutsch_sentences,GLOBALS[\"OUTPUT-VOCABULARY-SIZE\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "##########################################\n",
    "# Input Embedding Layer\n",
    "##########################################\n",
    "class InputEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_vector_dimension):\n",
    "        super(InputEmbeddingLayer, self).__init__()\n",
    "        # nn.Embedding registers the lookup table as a parameter\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_vector_dimension)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # Output shape : (seq_len, embedding_dim)\n",
    "        return self.embedding(tokens)\n",
    "\n",
    "##########################################\n",
    "# Positional Encoding Layer\n",
    "##########################################\n",
    "class PositionalEncodingLayer(nn.Module):\n",
    "    def __init__(self, sequence_length, embedding_vector_dimension):\n",
    "        super(PositionalEncodingLayer, self).__init__()\n",
    "        # Precompute positional encoding and register as buffer so it's moved to GPU with the model.\n",
    "        encoding = torch.zeros(sequence_length, embedding_vector_dimension)\n",
    "        position = torch.arange(0, sequence_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_vector_dimension, 2, dtype=torch.float) *\n",
    "                             -(math.log(10000.0) / embedding_vector_dimension))\n",
    "        encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Register as a buffer (non-trainable but saved with the model)\n",
    "        self.register_buffer('encoding', encoding)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # embeddings shape: (seq_len, embedding_dim) or broadcastable shape.\n",
    "        return embeddings + self.encoding\n",
    "\n",
    "##########################################\n",
    "# Multi-Head Attention\n",
    "##########################################\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # d_model is the embedding dimension; must be divisible by num_heads.\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V. Bias is often set to False.\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        # Final output projection.\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        # x shape: (batch, seq_len, d_model)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        # Reshape to (batch, seq_len, num_heads, head_dim)\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        # Transpose to (batch, num_heads, seq_len, head_dim)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        # x shape: (batch, num_heads, seq_len, head_dim)\n",
    "        batch_size, num_heads, seq_len, head_dim = x.size()\n",
    "        # Transpose to (batch, seq_len, num_heads, head_dim) then reshape to (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous()  # ensure contiguous memory\n",
    "        return x.view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Q, K, V shape: (batch, num_heads, seq_len, head_dim)\n",
    "        d_k = Q.size(-1)\n",
    "        # Compute attention scores: (batch, num_heads, seq_len, seq_len)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        # Softmax over the last dimension (keys)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)  # (batch, num_heads, seq_len, head_dim)\n",
    "        return attention_weights, output\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # x shape: (batch, seq_len, d_model)\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        # Split into heads.\n",
    "        Q = self.split_heads(Q)  # (batch, num_heads, seq_len, head_dim)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        # Compute attention.\n",
    "        attention_weights, attention_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        # Combine heads back.\n",
    "        attention_output = self.combine_heads(attention_output)  # (batch, seq_len, d_model)\n",
    "        # Final linear projection.\n",
    "        output = self.W_o(attention_output)\n",
    "        return attention_weights, output\n",
    "\n",
    "##########################################\n",
    "# Add & Norm Layer (Residual Connection + LayerNorm)\n",
    "##########################################\n",
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(AddAndNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)  # Normalizes over last dimension (d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer_output):\n",
    "        # Apply dropout to sublayer output, then add the residual (x) and normalize.\n",
    "        return self.norm(x + self.dropout(sublayer_output))\n",
    "\n",
    "##########################################\n",
    "# Position-wise Feed Forward Network (FFN)\n",
    "##########################################\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply two linear transformations with ReLU activation and dropout in between.\n",
    "        return self.linear2(self.dropout(self.relu(self.linear1(x))))\n",
    "\n",
    "##########################################\n",
    "# Encoder Block\n",
    "##########################################\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.add_and_norm1 = AddAndNorm(d_model, dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.add_and_norm2 = AddAndNorm(d_model, dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-head attention sublayer.\n",
    "        attention_weights, attention_output = self.mha(x, mask)\n",
    "        x = self.add_and_norm1(x, attention_output)\n",
    "        # Position-wise FFN sublayer.\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.add_and_norm2(x, ffn_output)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "##########################################\n",
    "# Encoder (Stack of Encoder Blocks)\n",
    "##########################################\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_blocks, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Using nn.ModuleList ensures submodules are registered.\n",
    "        self.blocks = nn.ModuleList([\n",
    "            EncoderBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Pass input through each encoder block sequentially.\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0169,  1.3837,  0.1888,  ...,  0.5041, -0.2973, -0.0765],\n",
      "         [-0.1123,  1.3558,  1.4663,  ...,  1.2477,  1.4175,  1.9930],\n",
      "         [ 1.5835, -1.4333,  1.1004,  ...,  0.6044, -0.9925,  0.6191],\n",
      "         ...,\n",
      "         [ 0.4850,  0.5302, -1.2330,  ...,  0.7893, -0.5802,  0.7583],\n",
      "         [ 1.2964,  0.0193, -1.3790,  ...,  0.7893, -0.5801,  0.7583],\n",
      "         [ 1.3049, -0.9395, -0.6666,  ...,  0.7893, -0.5800,  0.7583]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "vocab_size = GLOBALS['INPUT-VOCABULARY-SIZE']\n",
    "d_model = GLOBALS['INPUT-EMBEDDING-DIMENSION']\n",
    "seq_len = GLOBALS['CONTEXT-SIZE']\n",
    "input_embedding_layer = InputEmbeddingLayer(vocab_size,d_model)\n",
    "encoder_block_count = GLOBALS['ENCODER-BLOCK-COUNT']\n",
    "num_heads = GLOBALS['ATTENTION-HEAD-COUNT']\n",
    "ffn_hidden_dimension = GLOBALS['FFN-HIDDEN-DIMENSION']\n",
    "positional_encoding_layer = PositionalEncodingLayer(seq_len,d_model)\n",
    "\n",
    "encoder = Encoder(encoder_block_count,d_model,num_heads,ffn_hidden_dimension)\n",
    "\n",
    "# Create a causal mask with shape (1, 1, seq_len, seq_len)\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "for english_sentence,deutsch_sentence in zip(english_sentences,deutsch_sentences):\n",
    "    \n",
    "    english_encoding = english_encoder.encode(english_sentence).ids\n",
    "    deutsch_encoding = deutsch_encoder.encode(deutsch_sentence).ids\n",
    "    \n",
    "\n",
    "    input_embeddings = input_embedding_layer.forward(torch.LongTensor(english_encoding).reshape(1,-1))\n",
    "    \n",
    "    input_embeddings = positional_encoding_layer.forward(input_embeddings)\n",
    "    \n",
    "    x = input_embeddings\n",
    "    encoder.forward(x)\n",
    "    print(x)\n",
    "    \n",
    "    \n",
    "    # print(\"English: \",english_encoding)\n",
    "    # print(\"Deutsch: \",deutsch_encoding)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
