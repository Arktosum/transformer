{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"./train-00000-of-00001.parquet\")  # If using local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "english_sentences = []\n",
    "deutsch_sentences = []\n",
    "for i in range(len(df))[:100]:\n",
    "    english = df.iloc[i].iloc[0]['en']\n",
    "    deutsch = df.iloc[i].iloc[0]['de']\n",
    "    english_sentences.append(english)\n",
    "    deutsch_sentences.append(deutsch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBALS = {\n",
    "    \"INPUT-VOCABULARY-SIZE\" : 30_000, # number of accepted distinct INPUT tokens.\n",
    "    \"OUTPUT-VOCABULARY-SIZE\" : 30_000, # number of accepted distinct OUTPUT tokens.\n",
    "    \"D-MODEL\" : 512, # dimension of each token's embedding vector.\n",
    "    \"INPUT-SEQUENCE-LENGTH\" : 256,\n",
    "    \"NUM-HEADS\" : 4 ,# must divide D-MODEL evenly!\n",
    "    \"D-FF\" : 1024,\n",
    "    'NUM-ENCODER-BLOCKS' : 4,\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BPETokenizer(corpus_sentences,vocab_size=30000):\n",
    "    # Initialize a BPE tokenizer\n",
    "    tokenizer = Tokenizer(BPE())\n",
    "\n",
    "    # Set up a trainer with desired vocabulary size\n",
    "    trainer = BpeTrainer(vocab_size=vocab_size, min_frequency=2, special_tokens=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "\n",
    "    # Define a pre-tokenizer to split input text into words\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    # Tokenizer expects an iterator of strings\n",
    "    tokenizer.train_from_iterator(corpus_sentences, trainer=trainer)\n",
    "    tokenizer.enable_padding(length=GLOBALS['INPUT-SEQUENCE-LENGTH'], pad_id=tokenizer.token_to_id(\"<pad>\"), pad_token=\"<pad>\")\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "english_encoder = BPETokenizer(english_sentences,GLOBALS[\"INPUT-VOCABULARY-SIZE\"])\n",
    "deutsch_encoder = BPETokenizer(deutsch_sentences,GLOBALS[\"OUTPUT-VOCABULARY-SIZE\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        # The layer stores a learnable weight matrix of size (vocab_size, d_model).\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Tensor of shape (batch_size, input_context_length)\n",
    "        embedded = self.embedding(x)\n",
    "        # embedded: Tensor of shape (batch_size, input_context_length, d_model)\n",
    "        return embedded\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a matrix of shape (seq_length, d_model) to hold the positional encodings\n",
    "        pe = torch.zeros(seq_length, d_model)\n",
    "        # Position indices (0, 1, 2, ..., seq_length-1)\n",
    "        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        # Compute the div_term based on the dimension indices\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # Apply sine to even indices and cosine to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Add a batch dimension\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # Register as a buffer to prevent updates during training\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Tensor of shape (batch_size, input_context_length, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Learnable weight matrices for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output linear transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(x)  # (batch_size, seq_length, d_model)\n",
    "        K = self.W_k(x)  # (batch_size, seq_length, d_model)\n",
    "        V = self.W_v(x)  # (batch_size, seq_length, d_model)\n",
    "        \n",
    "        # Reshape Q, K, V for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, seq_length, d_k)\n",
    "        K = K.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, seq_length, d_k)\n",
    "        V = V.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, seq_length, d_k)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  # (batch_size, num_heads, seq_length, seq_length)\n",
    "        attention_weights = torch.nn.functional.softmax(scores, dim=-1)  # (batch_size, num_heads, seq_length, seq_length)\n",
    "        attention_output = torch.matmul(attention_weights, V)  # (batch_size, num_heads, seq_length, d_k)\n",
    "        \n",
    "        # Concatenate heads and apply final linear transformation\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)  # (batch_size, seq_length, d_model)\n",
    "        output = self.W_o(attention_output)  # (batch_size, seq_length, d_model)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer_output):\n",
    "        # Apply dropout to the sublayer output\n",
    "        sublayer_output = self.dropout(sublayer_output)\n",
    "        # Add the original input (residual connection) and normalize\n",
    "        return self.layer_norm(x + sublayer_output)\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        \"\"\"\n",
    "        Initializes the PositionwiseFeedForward layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the input and output features.\n",
    "            d_ff (int): Dimensionality of the hidden layer.\n",
    "            dropout (float): Dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        \n",
    "        # First linear transformation: projects from d_model to d_ff\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        \n",
    "        # ReLU activation function introduces non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Second linear transformation: projects back from d_ff to d_model\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the PositionwiseFeedForward layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, d_model).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of the same shape as input.\n",
    "        \"\"\"\n",
    "        # Apply first linear transformation\n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        # Apply ReLU activation\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Apply second linear transformation\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer Encoder Layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the input embeddings.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Dimensionality of the feed-forward network's hidden layer.\n",
    "            dropout (float): Dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        \n",
    "        # Multi-Head Self-Attention mechanism\n",
    "        self.self_attention = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        \n",
    "        # Add & Norm layer after self-attention\n",
    "        self.add_norm1 = AddNorm(d_model)\n",
    "        \n",
    "        # Position-wise Feed-Forward Network\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
    "        \n",
    "        # Add & Norm layer after feed-forward network\n",
    "        self.add_norm2 = AddNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Transformer Encoder Layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, d_model).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of the same shape as input.\n",
    "        \"\"\"\n",
    "        # Apply multi-head self-attention\n",
    "        attention_output = self.self_attention(x)\n",
    "        \n",
    "        # Apply Add & Norm\n",
    "        x = self.add_norm1(x, attention_output)\n",
    "        \n",
    "        # Apply position-wise feed-forward network\n",
    "        ff_output = self.feed_forward(x)\n",
    "        \n",
    "        # Apply Add & Norm\n",
    "        x = self.add_norm2(x, ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer Encoder.\n",
    "\n",
    "        Args:\n",
    "            num_layers (int): Number of encoder layers to stack.\n",
    "            d_model (int): Dimensionality of the input embeddings.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Dimensionality of the feed-forward network's hidden layer.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Stack of encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderBlock(d_model, num_heads, d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer normalization applied after the last encoder layer\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Transformer Encoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, d_model).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of the same shape as input.\n",
    "        \"\"\"\n",
    "        # Pass the input through each encoder layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Apply layer normalization to the final output\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "input_embedding_layer = InputEmbedding(GLOBALS['INPUT-VOCABULARY-SIZE'],GLOBALS['D-MODEL'])\n",
    "input_positional_encoding_layer = PositionalEncoding(GLOBALS['D-MODEL'],GLOBALS['INPUT-SEQUENCE-LENGTH'])\n",
    "\n",
    "encoder = Encoder(GLOBALS['NUM-ENCODER-BLOCKS'],GLOBALS['D-MODEL'],GLOBALS['NUM-HEADS'],GLOBALS[\"D-FF\"])\n",
    "\n",
    "for english_sentence,deutsch_sentence in zip(english_sentences,deutsch_sentences):\n",
    "    \n",
    "    english_encoding = english_encoder.encode(english_sentence).ids\n",
    "    \n",
    "    english_encoding = torch.LongTensor(english_encoding).reshape(1,-1)    # (batch,input_context_len)\n",
    "    x = input_embedding_layer.forward(english_encoding) # (batch_size, input_context_length, d_model)\n",
    "    x = input_positional_encoding_layer(x) # (batch_size, input_context_length, d_model)\n",
    "    \n",
    "    x = encoder.forward(x)\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
